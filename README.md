# capstone_project

**Cross-Cultural Annotator Disagreement in AI Safety Evaluation**.

## Overview

This project explores how human raters from different demographic backgrounds (e.g., gender, race, education) evaluate the safety of AI model outputs.  
Using a dataset of adversarial dialogues and rater responses, the goal is to:

- Cluster annotators based on demographic information and rating behavior  
- Measure disagreement within and across clusters using entropy  
- Identify the most agreed and most disagreed items  
- Visualize trends using word clouds and clustering plots

## Features

- **Clustering Techniques**: KMeans, KModes, HDBSCAN, and Hierarchical clustering  
- **Disagreement Quantification**: Entropy-based metrics for intra- and inter-cluster variance  
- **Word Cloud Generation**: For top agreed/disagreed contexts and model responses  
- **PDF/CSV Outputs**: Plots and tables saved for reproducible reporting
